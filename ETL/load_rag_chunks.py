import json
import re
from pathlib import Path

def main():
    txt_path = 'EDA/hipaa_linear_text.txt'
    stats_path = 'EDA/link_extraction_stats.json'
    output_path = 'data/hipaa_rag_chunks.json'

    if not Path(txt_path).exists():
        print("‚ùå Run 'python EDA/linearize_columns.py' first!")
        return

    print(f"üìÇ Loading section list...")
    with open(stats_path, 'r') as f:
        stats = json.load(f)
        target_sections = stats.get('all_sections_sorted', [])

    print(f"üìñ Reading linear text...")
    with open(txt_path, 'r', encoding='utf-8') as f:
        full_text = f.read()

    chunks = []
    # –ù–∞—á–∏–Ω–∞–µ–º –ø–æ–∏—Å–∫ —Å –Ω–∞—á–∞–ª–∞ —Ñ–∞–π–ª–∞
    current_search_idx = 0
    
    print(f"üîç Extracting {len(target_sections)} RAG chunks...")

    for i, section_id in enumerate(target_sections):
        s_esc = re.escape(section_id)
        
        # --- –ò–ó–ú–ï–ù–ï–ù–ò–ï 1: –£–ª—É—á—à–µ–Ω–Ω—ã–π Regex ---
        # (?m) - –≤–∫–ª—é—á–∞–µ—Ç –º–Ω–æ–≥–æ—Å—Ç—Ä–æ—á–Ω—ã–π —Ä–µ–∂–∏–º, —á—Ç–æ–±—ã ^ —Ä–∞–±–æ—Ç–∞–ª–æ –¥–ª—è –∫–∞–∂–¥–æ–π —Å—Ç—Ä–æ–∫–∏
        # ^ - –Ω–∞—á–∞–ª–æ —Å—Ç—Ä–æ–∫–∏ (—á—Ç–æ–±—ã –Ω–µ –Ω–∞—Ö–æ–¥–∏—Ç—å "see ¬ß 162.510" –≤–Ω—É—Ç—Ä–∏ —Ç–µ–∫—Å—Ç–∞)
        # (?:¬ß|Section) - –∏—â–µ–º —Å–ª–æ–≤–æ Section –∏–ª–∏ –∑–Ω–∞–∫ ¬ß
        # \s+ - –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –æ–∂–∏–¥–∞–µ–º –ø—Ä–æ–±–µ–ª(—ã) –ø–æ—Å–ª–µ –∑–Ω–∞–∫–∞
        pattern = rf'(?m)^(?:¬ß|Section)\s+{s_esc}(?:\s|\.|,|\(|$)'
        
        match = re.search(pattern, full_text[current_search_idx:])
        
        found_start = -1
        
        if match:
            # –ê–±—Å–æ–ª—é—Ç–Ω–∞—è –ø–æ–∑–∏—Ü–∏—è
            found_start = current_search_idx + match.start()
            
            # --- –ò–ó–ú–ï–ù–ï–ù–ò–ï 2: –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –û–≥–ª–∞–≤–ª–µ–Ω–∏–µ (TOC) ---
            # –ï—Å–ª–∏ —Å—Ç—Ä–æ–∫–∞ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∞—è –∏–ª–∏ —Å–æ–¥–µ—Ä–∂–∏—Ç –º–Ω–æ–≥–æ —Ç–æ—á–µ–∫/—Ü–∏—Ñ—Ä –≤ –∫–æ–Ω—Ü–µ ‚Äî —ç—Ç–æ, –≤–µ—Ä–æ—è—Ç–Ω–æ, TOC
            line_end = full_text.find('\n', found_start)
            header_line = full_text[found_start:line_end].strip()
            
            # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞: –µ—Å–ª–∏ –≤ —Å—Ç—Ä–æ–∫–µ –µ—Å—Ç—å "....." –∏–ª–∏ –æ–Ω–∞ –∑–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è —á–∏—Å–ª–æ–º (—Å—Ç—Ä–∞–Ω–∏—Ü–µ–π)
            if "..." in header_line or re.search(r'\.\s*\d+$', header_line):
                # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ —Å–ª–µ–¥—É—é—â–µ–µ –≤—Ö–æ–∂–¥–µ–Ω–∏–µ
                retry_match = re.search(pattern, full_text[found_start + len(header_line):])
                if retry_match:
                    found_start = found_start + len(header_line) + retry_match.start()

            # –ò—â–µ–º –∫–æ–Ω–µ—Ü —Ç–µ–∫—É—â–µ–π —Å–µ–∫—Ü–∏–∏ (–Ω–∞—á–∞–ª–æ —Å–ª–µ–¥—É—é—â–µ–π –∏–∑ —Å–ø–∏—Å–∫–∞)
            end_pos = -1
            
            # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –Ω–∞—á–∞–ª–æ –õ–Æ–ë–û–ô —Å–ª–µ–¥—É—é—â–µ–π —Å–µ–∫—Ü–∏–∏, —á—Ç–æ–±—ã –Ω–µ –∑–∞–≤–∏—Å–µ—Ç—å —Ç–æ–ª—å–∫–æ –æ—Ç i+1
            # (–Ω–∞ —Å–ª—É—á–∞–π –µ—Å–ª–∏ 162.512 –ø—Ä–æ–ø—É—â–µ–Ω–∞, –Ω–æ –µ—Å—Ç—å 162.514)
            if i + 1 < len(target_sections):
                next_id = target_sections[i+1]
                n_esc = re.escape(next_id)
                # –¢–∞–∫–æ–π –∂–µ —Å—Ç—Ä–æ–≥–∏–π –ø–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–π —Å–µ–∫—Ü–∏–∏
                next_pattern = rf'(?m)^(?:¬ß|Section)\s+{n_esc}(?:\s|\.|,|\(|$)'
                
                next_match = re.search(next_pattern, full_text[found_start + 50:]) # +50 –±–∞–π—Ç, —á—Ç–æ–±—ã –Ω–µ –Ω–∞–π—Ç–∏ —Å–∞–º—É —Å–µ–±—è
                if next_match:
                    end_pos = found_start + 50 + next_match.start()
            
            # Fallback: –ï—Å–ª–∏ —Å–ª–µ–¥—É—é—â–∞—è —Å–µ–∫—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ (–∏–ª–∏ —ç—Ç–æ –ø–æ—Å–ª–µ–¥–Ω—è—è), 
            # –∏—â–µ–º –ø—Ä–æ—Å—Ç–æ —Å–ª–µ–¥—É—é—â–∏–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ –≤–∏–¥–∞ "¬ß 1..." –∫–∞–∫ –≥—Ä–∞–Ω–∏—Ü—É
            if end_pos == -1:
                 # –ò—â–µ–º –ª—é–±–æ–π —Å–ª–µ–¥—É—é—â–∏–π –ø–∞—Ä–∞–≥—Ä–∞—Ñ, –Ω–∞—á–∏–Ω–∞—é—â–∏–π—Å—è —Å –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏
                 generic_next = re.search(r'(?m)^(?:¬ß|Section)\s+\d+\.\d+', full_text[found_start + 200:])
                 if generic_next:
                     end_pos = found_start + 200 + generic_next.start()
                 else:
                     end_pos = min(len(full_text), found_start + 20000) # –ú–∞–∫—Å–∏–º—É–º 20–∫ —Å–∏–º–≤–æ–ª–æ–≤ –Ω–∞ —Å–µ–∫—Ü–∏—é

            content = full_text[found_start:end_pos].strip()
            
            chunks.append({
                "id": section_id,
                "text": content,
                "metadata": {
                    "source": "hipaa_combined.pdf",
                    "section": section_id,
                    "part": section_id.split('.')[0]
                }
            })
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –∏–Ω–¥–µ–∫—Å –ø–æ–∏—Å–∫–∞, —á—Ç–æ–±—ã —Å–ª–µ–¥—É—é—â—É—é —Å–µ–∫—Ü–∏—é –∏—Å–∫–∞—Ç—å –ü–û–°–õ–ï —Ç–µ–∫—É—â–µ–π
            # –í–∞–∂–Ω–æ: —Å—Ç–∞–≤–∏–º –∫—É—Ä—Å–æ—Ä —á—É—Ç—å –¥–∞–ª—å—à–µ –Ω–∞—á–∞–ª–∞ —Ç–µ–∫—É—â–µ–π, –Ω–æ –Ω–µ –≤ —Å–∞–º—ã–π –∫–æ–Ω–µ—Ü, 
            # –Ω–∞ —Å–ª—É—á–∞–π –µ—Å–ª–∏ "–∫–æ–Ω–µ—Ü" –æ–ø—Ä–µ–¥–µ–ª–µ–Ω –Ω–µ–≤–µ—Ä–Ω–æ.
            # –ù–æ –ª—É—á—à–µ —Å—Ç–∞–≤–∏—Ç—å –≤ –Ω–∞—á–∞–ª–æ –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ + 1, —á—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –ø–æ—Ä—è–¥–æ–∫.
            current_search_idx = found_start + 1
            
        else:
            print(f"   ‚ö†Ô∏è Text for ¬ß {section_id} not found starting from idx {current_search_idx}")
            # –í–ê–ñ–ù–û: –ù–µ –¥–≤–∏–≥–∞–µ–º current_search_idx, –µ—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏. 
            # –í–æ–∑–º–æ–∂–Ω–æ, –º—ã –ø—Ä–æ–ø—É—Å—Ç–∏–ª–∏ —Å–µ–∫—Ü–∏—é, –Ω–æ —Å–ª–µ–¥—É—é—â–∞—è (i+1) –≤—Å–µ –µ—â–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–∞–π–¥–µ–Ω–∞ –¥–∞–ª—å—à–µ.

    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(chunks, f, indent=2, ensure_ascii=False)

    print(f"\n‚úÖ Created {len(chunks)} RAG-ready chunks")
    print(f"ÔøΩ Saved to: {output_path}")

if __name__ == "__main__":
    main()