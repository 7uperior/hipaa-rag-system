"""
HIPAA Vector Embedding Loader (Async) - Legal RAG Optimized
–û–±–Ω–æ–≤–ª–µ–Ω–æ –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ–¥—á–∞–Ω–∫–æ–≤
"""

import os
import json
import asyncio
import asyncpg
from openai import AsyncOpenAI
from pgvector.asyncpg import register_vector
from typing import Optional, Dict, Any

# Initialize Async Client
client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# === –ö–û–ù–°–¢–ê–ù–¢–´ ===
EMBEDDING_MODEL = "text-embedding-3-small"
EMBEDDING_DIMENSION = 1536
MAX_TEXT_LENGTH = 8000  # –£–∂–µ –Ω–µ –Ω—É–∂–Ω–æ —É—Ä–µ–∑–∞—Ç—å - –≤—Å–µ —á–∞–Ω–∫–∏ < 8000
BATCH_SIZE = 10

# === –£–¢–ò–õ–ò–¢–´ ===

async def get_embedding(text: str) -> Optional[list]:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è embedding —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫."""
    try:
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π
        if len(text) > MAX_TEXT_LENGTH:
            print(f"‚ö†Ô∏è Text too long ({len(text)} chars), truncating to {MAX_TEXT_LENGTH}")
            text = text[:MAX_TEXT_LENGTH]
        
        response = await client.embeddings.create(
            model=EMBEDDING_MODEL,
            input=text
        )
        return response.data[0].embedding
    except Exception as e:
        print(f"‚ö†Ô∏è Error generating embedding: {e}")
        return None


def prepare_embedding_text(chunk: Dict[str, Any]) -> str:
    """
    –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è embedding —Å —É—á–µ—Ç–æ–º —Ç–∏–ø–∞ —á–∞–Ω–∫–∞.
    –î–æ–±–∞–≤–ª—è–µ—Ç –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–∏—Å–∫–∞.
    
    –û–ë–ù–û–í–õ–ï–ù–û: –£—á–∏—Ç—ã–≤–∞–µ—Ç –ø–æ–¥—á–∞–Ω–∫–∏ —Å –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–æ–π.
    """
    chunk_type = chunk['type']
    parts = []
    
    # 1. SECTION - —Å–∞–º—ã–π –≤–∞–∂–Ω—ã–π —Ç–∏–ø
    if chunk_type == 'section':
        # –ò–µ—Ä–∞—Ä—Ö–∏—è: Part -> Subpart -> Section
        if chunk.get('part_title'):
            parts.append(f"Part {chunk['part']}: {chunk['part_title']}")
        
        if chunk.get('subpart_title'):
            subpart = chunk.get('subpart', '')
            parts.append(f"Subpart {subpart}: {chunk['subpart_title']}")
        
        # –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å–µ–∫—Ü–∏–∏ (–º–æ–∂–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–æ–¥—Ä–∞–∑–¥–µ–ª–∞—Ö)
        if chunk.get('section_title'):
            parts.append(f"{chunk['section']}: {chunk['section_title']}")
        
        # –ù–û–í–û–ï: –ï—Å–ª–∏ —ç—Ç–æ –ø–æ–¥—á–∞–Ω–∫, –¥–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
        if chunk.get('is_subchunk'):
            parent = chunk.get('parent_section')
            marker = chunk.get('subsection_marker')
            if parent and marker:
                parts.append(f"Part of {parent}, subsection {marker}")
        
        # –û—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç
        parts.append(chunk['text'])
    
    # 2. PART_METADATA - –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —á–∞—Å—Ç–∏
    elif chunk_type == 'part_metadata':
        parts.append(f"Part {chunk['part']}: {chunk['part_title']}")
        
        if chunk.get('authority'):
            parts.append(f"Legal Authority: {chunk['authority']}")
        
        if chunk.get('source'):
            parts.append(f"Source: {chunk['source']}")
        
        parts.append(chunk['text'])
    
    # 3. SUBPART_METADATA
    elif chunk_type == 'subpart_metadata':
        parts.append(f"Part {chunk['part']}")
        parts.append(f"Subpart {chunk['subpart']}: {chunk.get('subpart_title', '')}")
        
        if chunk.get('source'):
            parts.append(f"Source: {chunk['source']}")
        
        parts.append(chunk['text'])
    
    # 4 & 5. RESERVED (–º–æ–∂–Ω–æ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å –∏–ª–∏ —Å–¥–µ–ª–∞—Ç—å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π embedding)
    else:  # reserved_section –∏–ª–∏ reserved_subpart
        parts.append(f"Part {chunk['part']}")
        parts.append(f"Reserved: {chunk.get('section', chunk.get('subpart', ''))}")
        parts.append(chunk['text'])
    
    return "\n".join(parts)


def safe_get(chunk: Dict, key: str, default=None):
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç None –¥–ª—è –ø—É—Å—Ç—ã—Ö —Å—Ç—Ä–æ–∫."""
    value = chunk.get(key, default)
    return value if value not in (None, '', []) else default


async def create_schema(conn: asyncpg.Connection):
    """–°–æ–∑–¥–∞–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å—Ö–µ–º—ã –ë–î —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –ø–æ–¥—á–∞–Ω–∫–æ–≤."""
    
    print("üì¶ Creating pgvector extension...")
    await conn.execute("CREATE EXTENSION IF NOT EXISTS vector;")
    await register_vector(conn)
    
    print("üì¶ Creating table schema...")
    await conn.execute("""
        DROP TABLE IF EXISTS hipaa_sections CASCADE;
        
        CREATE TABLE hipaa_sections (
            -- Primary key
            id SERIAL PRIMARY KEY,
            
            -- –£–Ω–∏–∫–∞–ª—å–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —á–∞–Ω–∫–∞
            chunk_id VARCHAR(100) UNIQUE NOT NULL,
            
            -- –¢–∏–ø —á–∞–Ω–∫–∞ (–¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏)
            chunk_type VARCHAR(50) NOT NULL,
            
            -- === –ò–ï–†–ê–†–•–ò–ß–ï–°–ö–ê–Ø –°–¢–†–£–ö–¢–£–†–ê ===
            part VARCHAR(10) NOT NULL,
            part_title TEXT,
            
            subpart VARCHAR(50),
            subpart_title TEXT,
            
            section VARCHAR(50),
            section_title TEXT,
            
            -- === –ù–û–í–´–ï –ü–û–õ–Ø –î–õ–Ø –ü–û–î–ß–ê–ù–ö–û–í ===
            is_subchunk BOOLEAN DEFAULT FALSE,
            parent_section VARCHAR(100),
            subsection_marker VARCHAR(20),
            chunk_part VARCHAR(20),
            grouped_subsections TEXT[],
            group_index INTEGER,
            
            -- === –ö–û–ù–¢–ï–ù–¢ ===
            text TEXT NOT NULL,
            
            -- === –ú–ï–¢–ê–î–ê–ù–ù–´–ï (–¥–ª—è part_metadata) ===
            authority TEXT,
            source TEXT,
            
            -- === –ü–ï–†–ï–ö–†–ï–°–¢–ù–´–ï –°–°–´–õ–ö–ò ===
            cross_references TEXT[],
            
            -- === –í–ï–ö–¢–û–†–ù–û–ï –ü–†–ï–î–°–¢–ê–í–õ–ï–ù–ò–ï ===
            embedding vector(1536),
            
            -- === –°–õ–£–ñ–ï–ë–ù–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø ===
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            
            -- === CONSTRAINTS ===
            CONSTRAINT valid_chunk_type CHECK (
                chunk_type IN ('section', 'part_metadata', 'subpart_metadata', 
                              'reserved_section', 'reserved_subpart')
            )
        );
    """)
    
    print("üìä Creating indexes...")
    await conn.execute("""
        -- –ò–Ω–¥–µ–∫—Å—ã –¥–ª—è –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞
        CREATE INDEX idx_part ON hipaa_sections(part);
        CREATE INDEX idx_subpart ON hipaa_sections(part, subpart);
        CREATE INDEX idx_section ON hipaa_sections(section) WHERE section IS NOT NULL;
        CREATE INDEX idx_chunk_type ON hipaa_sections(chunk_type);
        
        -- –ò–Ω–¥–µ–∫—Å—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –ø–æ–¥—á–∞–Ω–∫–∞–º–∏
        CREATE INDEX idx_is_subchunk ON hipaa_sections(is_subchunk);
        CREATE INDEX idx_parent_section ON hipaa_sections(parent_section) 
            WHERE parent_section IS NOT NULL;
        CREATE INDEX idx_subsection_marker ON hipaa_sections(subsection_marker)
            WHERE subsection_marker IS NOT NULL;
        CREATE INDEX idx_group_index ON hipaa_sections(group_index)
            WHERE group_index IS NOT NULL;
        
        -- –ö–æ–º–ø–æ–∑–∏—Ç–Ω—ã–π –∏–Ω–¥–µ–∫—Å –¥–ª—è —á–∞—Å—Ç—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        CREATE INDEX idx_part_type ON hipaa_sections(part, chunk_type);
        
        -- –ü–æ–ª–Ω–æ—Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫ (–¥–ª—è –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ RAG)
        CREATE INDEX idx_text_fts ON hipaa_sections 
            USING gin(to_tsvector('english', text));
        
        -- –ü–æ–∏—Å–∫ –ø–æ –ø–µ—Ä–µ–∫—Ä–µ—Å—Ç–Ω—ã–º —Å—Å—ã–ª–∫–∞–º
        CREATE INDEX idx_cross_references_gin ON hipaa_sections 
            USING gin(cross_references)
            WHERE cross_references IS NOT NULL AND array_length(cross_references, 1) > 0;
    """)
    
    print("‚úÖ Schema created!")


async def load_chunks(conn: asyncpg.Connection, json_path: str, 
                     skip_reserved: bool = True):
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ —á–∞–Ω–∫–æ–≤ –≤ –ë–î —Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π embeddings.
    –û–ë–ù–û–í–õ–ï–ù–û: –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –Ω–æ–≤—ã—Ö –ø–æ–ª–µ–π –ø–æ–¥—á–∞–Ω–∫–æ–≤.
    
    Args:
        conn: –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î
        json_path: –ü—É—Ç—å –∫ JSON —Ñ–∞–π–ª—É —Å —á–∞–Ω–∫–∞–º–∏
        skip_reserved: –ü—Ä–æ–ø—É—Å–∫–∞—Ç—å –ª–∏ reserved —á–∞–Ω–∫–∏ (—ç–∫–æ–Ω–æ–º–∏—Ç —Ç–æ–∫–µ–Ω—ã)
    """
    
    print(f"üìñ Loading chunks from {json_path}...")
    with open(json_path, 'r', encoding='utf-8') as f:
        chunks = json.load(f)
    
    print(f"Found {len(chunks)} chunks")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ø–æ–¥—á–∞–Ω–∫–∞–º
    subchunks = [c for c in chunks if c.get('is_subchunk')]
    print(f"   ‚Ä¢ Regular chunks: {len(chunks) - len(subchunks)}")
    print(f"   ‚Ä¢ Subchunks: {len(subchunks)}")
    
    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è reserved (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    if skip_reserved:
        original_count = len(chunks)
        chunks = [c for c in chunks if c['type'] not in ['reserved_section', 'reserved_subpart']]
        print(f"Filtered out {original_count - len(chunks)} reserved chunks")
    
    total_chunks = len(chunks)
    print(f"Will process {total_chunks} chunks")
    
    print("üßÆ Generating embeddings and inserting into database...")
    
    successful_inserts = 0
    failed_inserts = 0
    
    for i in range(0, total_chunks, BATCH_SIZE):
        batch = chunks[i:i+BATCH_SIZE]
        
        # 1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è embedding
        texts_for_embedding = [prepare_embedding_text(chunk) for chunk in batch]
        
        # 2. –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è embeddings
        tasks = [get_embedding(text) for text in texts_for_embedding]
        embeddings = await asyncio.gather(*tasks)
        
        # 3. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è bulk insert
        insert_data = []
        for chunk, emb in zip(batch, embeddings):
            if emb is None:
                failed_inserts += 1
                continue
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å —É—á–µ—Ç–æ–º –ù–û–í–´–• –ø–æ–ª–µ–π
            insert_data.append((
                chunk['chunk_id'],
                chunk['type'],
                chunk['part'],
                safe_get(chunk, 'part_title'),
                safe_get(chunk, 'subpart'),
                safe_get(chunk, 'subpart_title'),
                safe_get(chunk, 'section'),
                safe_get(chunk, 'section_title'),
                # –ù–û–í–´–ï –ü–û–õ–Ø
                chunk.get('is_subchunk', False),
                safe_get(chunk, 'parent_section'),
                safe_get(chunk, 'subsection_marker'),
                safe_get(chunk, 'chunk_part'),
                safe_get(chunk, 'grouped_subsections', []),
                chunk.get('group_index'),
                # –û–°–¢–ê–õ–¨–ù–û–ï
                chunk['text'],
                safe_get(chunk, 'authority'),
                safe_get(chunk, 'source'),
                safe_get(chunk, 'references', []),
                emb
            ))
        
        # 4. Bulk insert
        if insert_data:
            try:
                await conn.executemany("""
                    INSERT INTO hipaa_sections (
                        chunk_id, chunk_type, part, part_title, 
                        subpart, subpart_title, section, section_title,
                        is_subchunk, parent_section, subsection_marker, 
                        chunk_part, grouped_subsections, group_index,
                        text, authority, source, cross_references, embedding
                    )
                    VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15, $16, $17, $18, $19)
                """, insert_data)
                successful_inserts += len(insert_data)
            except Exception as e:
                print(f"‚ùå Error inserting batch: {e}")
                failed_inserts += len(insert_data)
        
        print(f"  ‚úÖ Processed {min(i + BATCH_SIZE, total_chunks)}/{total_chunks} chunks...")
    
    print(f"\n‚úÖ Chunk loading complete!")
    print(f"   Success: {successful_inserts}")
    print(f"   Failed: {failed_inserts}")


async def create_vector_index(conn: asyncpg.Connection):
    """–°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ similarity search."""
    
    print("üìä Creating vector index (this may take a minute)...")
    
    await conn.execute("""
        CREATE INDEX hipaa_embedding_ivfflat_idx ON hipaa_sections 
        USING ivfflat (embedding vector_cosine_ops)
        WITH (lists = 100);
    """)
    
    print("‚úÖ Vector index created!")


async def create_helper_functions(conn: asyncpg.Connection):
    """
    –°–æ–∑–¥–∞–Ω–∏–µ –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π –¥–ª—è —É–ø—Ä–æ—â–µ–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∏ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è.
    """
    
    print("üîß Creating helper functions...")
    
    # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø–æ–ª–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø–æ–¥—á–∞–Ω–∫–∞
    await conn.execute("""
        CREATE OR REPLACE FUNCTION get_full_section_context(input_chunk_id VARCHAR)
        RETURNS TABLE (
            chunk_id VARCHAR,
            section VARCHAR,
            section_title TEXT,
            subsection_marker VARCHAR,
            text TEXT,
            is_main_chunk BOOLEAN
        ) AS $$
        BEGIN
            -- –ï—Å–ª–∏ —ç—Ç–æ –ø–æ–¥—á–∞–Ω–∫, –Ω–∞—Ö–æ–¥–∏–º –≤—Å–µ —á–∞—Å—Ç–∏ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–π —Å–µ–∫—Ü–∏–∏
            IF EXISTS (SELECT 1 FROM hipaa_sections WHERE chunk_id = input_chunk_id AND is_subchunk = TRUE) THEN
                RETURN QUERY
                SELECT 
                    h.chunk_id,
                    h.section,
                    h.section_title,
                    h.subsection_marker,
                    h.text,
                    h.chunk_id = input_chunk_id AS is_main_chunk
                FROM hipaa_sections h
                WHERE h.parent_section = (
                    SELECT parent_section FROM hipaa_sections WHERE chunk_id = input_chunk_id
                )
                OR h.chunk_id = (
                    SELECT parent_section FROM hipaa_sections WHERE chunk_id = input_chunk_id
                )
                ORDER BY h.chunk_id;
            ELSE
                -- –ï—Å–ª–∏ —ç—Ç–æ –æ–±—ã—á–Ω–∞—è —Å–µ–∫—Ü–∏—è, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ –µ—ë
                RETURN QUERY
                SELECT 
                    h.chunk_id,
                    h.section,
                    h.section_title,
                    h.subsection_marker,
                    h.text,
                    TRUE AS is_main_chunk
                FROM hipaa_sections h
                WHERE h.chunk_id = input_chunk_id;
            END IF;
        END;
        $$ LANGUAGE plpgsql;
    """)
    
    # –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Ü–∏—Ç–∞—Ç—ã
    await conn.execute("""
        CREATE OR REPLACE FUNCTION format_citation(
            input_section VARCHAR,
            input_subsection_marker VARCHAR DEFAULT NULL,
            input_part VARCHAR DEFAULT NULL
        )
        RETURNS TEXT AS $$
        BEGIN
            IF input_subsection_marker IS NOT NULL THEN
                RETURN input_section || ' ' || input_subsection_marker;
            ELSE
                RETURN input_section;
            END IF;
        END;
        $$ LANGUAGE plpgsql;
    """)
    
    print("‚úÖ Helper functions created!")


async def print_statistics(conn: asyncpg.Connection):
    """–í—ã–≤–æ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–º –¥–∞–Ω–Ω—ã–º."""
    
    print("\n" + "="*70)
    print("üìà DATABASE STATISTICS")
    print("="*70)
    
    # –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
    total = await conn.fetchval("SELECT COUNT(*) FROM hipaa_sections;")
    print(f"\nüìä Total chunks: {total}")
    
    # –ü–æ —Ç–∏–ø–∞–º
    print("\nüìã Chunks by type:")
    types = await conn.fetch("""
        SELECT chunk_type, COUNT(*) as cnt 
        FROM hipaa_sections 
        GROUP BY chunk_type
        ORDER BY cnt DESC;
    """)
    for row in types:
        print(f"   ‚Ä¢ {row['chunk_type']:<20} {row['cnt']:>5} chunks")
    
    # –ù–û–í–û–ï: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ø–æ–¥—á–∞–Ω–∫–∞–º
    print("\nüì¶ Subchunk statistics:")
    subchunk_stats = await conn.fetch("""
        SELECT 
            is_subchunk,
            COUNT(*) as cnt,
            AVG(LENGTH(text))::INT as avg_length,
            MAX(LENGTH(text)) as max_length
        FROM hipaa_sections
        WHERE chunk_type = 'section'
        GROUP BY is_subchunk;
    """)
    for row in subchunk_stats:
        chunk_type = "Subchunks" if row['is_subchunk'] else "Regular chunks"
        print(f"   ‚Ä¢ {chunk_type:<20} {row['cnt']:>5} (avg: {row['avg_length']:>5} chars, max: {row['max_length']:>5})")
    
    # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ–¥—á–∞–Ω–∫–∏
    grouped_count = await conn.fetchval("""
        SELECT COUNT(*) FROM hipaa_sections 
        WHERE grouped_subsections IS NOT NULL AND array_length(grouped_subsections, 1) > 1;
    """)
    print(f"   ‚Ä¢ Grouped subchunks:  {grouped_count:>5} (multiple subsections combined)")
    
    # –ü–æ —á–∞—Å—Ç—è–º
    print("\nüìö Chunks by Part:")
    parts = await conn.fetch("""
        SELECT part, part_title, COUNT(*) as cnt 
        FROM hipaa_sections 
        GROUP BY part, part_title
        ORDER BY part;
    """)
    for row in parts:
        title = row['part_title'] if row['part_title'] else 'N/A'
        if title and title != 'N/A':
            title = title[:50] + '...' if len(title) > 50 else title
        print(f"   ‚Ä¢ Part {row['part']:<4} {title:<55} {row['cnt']:>4} chunks")
    
    # –ü–µ—Ä–µ–∫—Ä–µ—Å—Ç–Ω—ã–µ —Å—Å—ã–ª–∫–∏
    refs_count = await conn.fetchval("""
        SELECT COUNT(*) FROM hipaa_sections 
        WHERE cross_references IS NOT NULL AND array_length(cross_references, 1) > 0;
    """)
    print(f"\nüîó Chunks with cross-references: {refs_count}")
    
    # –°–∞–º—ã–µ —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å–µ–∫—Ü–∏–∏
    if refs_count > 0:
        print("\nüîó Most referenced sections:")
        top_refs = await conn.fetch("""
            SELECT unnest(cross_references) as ref_section, COUNT(*) as ref_count
            FROM hipaa_sections
            WHERE cross_references IS NOT NULL
            GROUP BY ref_section
            ORDER BY ref_count DESC
            LIMIT 5;
        """)
        for row in top_refs:
            print(f"   ‚Ä¢ ¬ß {row['ref_section']:<15} referenced {row['ref_count']} times")
    
    print("\n" + "="*70)


async def print_example_queries(conn: asyncpg.Connection):
    """–ü—Ä–∏–º–µ—Ä—ã –ø–æ–ª–µ–∑–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –¥–∞–Ω–Ω—ã–º–∏."""
    
    print("\n" + "="*70)
    print("üí° EXAMPLE QUERIES")
    print("="*70)
    
    print("\n1Ô∏è‚É£ Semantic Search (with vector similarity):")
    print("""
    SELECT 
        chunk_id,
        section,
        subsection_marker,
        section_title,
        LEFT(text, 100) || '...' as preview,
        1 - (embedding <=> $1::vector) as similarity
    FROM hipaa_sections
    WHERE chunk_type = 'section'
    ORDER BY embedding <=> $1::vector
    LIMIT 5;
    """)
    
    print("\n2Ô∏è‚É£ Get full context of a subchunk:")
    print("""
    SELECT * FROM get_full_section_context('164.512_sub_g0_c_i');
    """)
    
    print("\n3Ô∏è‚É£ Find all parts of a split section:")
    print("""
    SELECT 
        chunk_id,
        subsection_marker,
        grouped_subsections,
        group_index,
        LENGTH(text) as text_length
    FROM hipaa_sections
    WHERE parent_section = '164.512'
    ORDER BY group_index, chunk_id;
    """)
    
    print("\n4Ô∏è‚É£ Search with citation formatting:")
    print("""
    SELECT 
        format_citation(section, subsection_marker, part) as citation,
        section_title,
        LEFT(text, 150) || '...' as preview
    FROM hipaa_sections
    WHERE chunk_type = 'section'
      AND to_tsvector('english', text) @@ plainto_tsquery('english', 'patient authorization')
    LIMIT 5;
    """)
    
    print("\n5Ô∏è‚É£ Hybrid search (vector + keyword):")
    print("""
    WITH vector_results AS (
        SELECT chunk_id, 1 - (embedding <=> $1::vector) as vector_score
        FROM hipaa_sections
        ORDER BY embedding <=> $1::vector
        LIMIT 20
    ),
    keyword_results AS (
        SELECT chunk_id, ts_rank(to_tsvector('english', text), plainto_tsquery('english', $2)) as keyword_score
        FROM hipaa_sections
        WHERE to_tsvector('english', text) @@ plainto_tsquery('english', $2)
    )
    SELECT 
        h.chunk_id,
        h.section,
        h.subsection_marker,
        h.section_title,
        COALESCE(v.vector_score, 0) * 0.7 + COALESCE(k.keyword_score, 0) * 0.3 as combined_score
    FROM hipaa_sections h
    LEFT JOIN vector_results v ON h.chunk_id = v.chunk_id
    LEFT JOIN keyword_results k ON h.chunk_id = k.chunk_id
    WHERE v.chunk_id IS NOT NULL OR k.chunk_id IS NOT NULL
    ORDER BY combined_score DESC
    LIMIT 5;
    """)
    
    print("\n" + "="*70)


async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è."""
    
    print("="*70)
    print("üè• HIPAA Legal RAG - Database Loader (Updated for Grouped Chunks)")
    print("="*70)
    
    # === –ü–û–î–ö–õ–Æ–ß–ï–ù–ò–ï –ö –ë–î ===
    print("\nüîå Connecting to database...")
    try:
        conn = await asyncpg.connect(
            host=os.getenv("DB_HOST", "postgres"),
            database=os.getenv("DB_NAME", "hipaa"),
            user=os.getenv("DB_USER", "user"),
            password=os.getenv("DB_PASSWORD", "pass")
        )
        print("‚úÖ Connected!")
    except Exception as e:
        print(f"‚ùå Connection failed: {e}")
        return
    
    try:
        # === –°–û–ó–î–ê–ù–ò–ï –°–•–ï–ú–´ ===
        await create_schema(conn)
        
        # === –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• ===
        json_path = '/app/hipaa_chunks.json'
        if not os.path.exists(json_path):
            # –ü—Ä–æ–±—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –ø—É—Ç–∏
            for alt_path in [
                '/app/hipaa_chunks_grouped.json',
                '/app/hipaa_data.json', 
                'hipaa_chunks.json',
                'hipaa_chunks_grouped.json',
                'hipaa_data.json'
            ]:
                if os.path.exists(alt_path):
                    json_path = alt_path
                    break
        
        print(f"üìÇ Using file: {json_path}")
        await load_chunks(conn, json_path, skip_reserved=True)
        
        # === –°–û–ó–î–ê–ù–ò–ï –í–ï–ö–¢–û–†–ù–û–ì–û –ò–ù–î–ï–ö–°–ê ===
        await create_vector_index(conn)
        
        # === –°–û–ó–î–ê–ù–ò–ï –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–• –§–£–ù–ö–¶–ò–ô ===
        await create_helper_functions(conn)
        
        # === –°–¢–ê–¢–ò–°–¢–ò–ö–ê ===
        await print_statistics(conn)
        
        # === –ü–†–ò–ú–ï–†–´ –ó–ê–ü–†–û–°–û–í ===
        await print_example_queries(conn)
        
    finally:
        await conn.close()
        print("\nüîí Connection closed")
        print("="*70)


if __name__ == "__main__":
    asyncio.run(main())